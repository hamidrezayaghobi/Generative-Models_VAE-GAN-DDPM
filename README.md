# VAE-GAN-DDPM

Welcome to the Image Generation repository! In this project, I've implemented three powerful generative models: Variational Autoencoder (VAE), Generative Adversarial Network (GAN), and Diffusion Probabilistic Models (DDPM). Using these models, I've generated captivating new images from the Fashion-MNIST dataset.
![Screenshot from 2023-09-14 21-07-01](https://github.com/hamidrezayaghobi/VAE-GAN-DDPM/assets/59170724/14d0ed6f-4147-4072-9440-8bba36cbf060) ,</br>


## Dataset
I've used the Fashion-MNIST dataset as the source of inspiration for generating new images. Fashion-MNIST is a collection of fashion items, making it an exciting choice for creative image generation.

## Variational Autoencoder (VAE)
VAE is a generative model consisting of an encoder network and a decoder network. The encoder maps the input data into a latent space, where probability distributions represent the data. A decoder network generates the output data from samples of the latent space.

VAE is trained using a variational inference approach, where the goal is to maximize the evidence lower bound (ELBO) of the log-likelihood of the data. The ELBO consists of the reconstruction loss, which measures how well the decoder can reconstruct the input data from the latent space, and the KL divergence between the prior (considered Gaussian) and posterior distributions over the latent space. The posterior distribution is usually considered Gaussian, and the encoder only tries to learn its mean and variance. Given the mean and the variance, one can use the reparameterization trick to sample from the encoder.
### VAE Examples
Interpolated between two randomly chosen points in the latent space, and choose 10 equally distant latent points between the random points
![Screenshot from 2023-09-14 21-11-23](https://github.com/hamidrezayaghobi/VAE-GAN-DDPM/assets/59170724/eaaa14a6-ac02-4de1-ad11-07e34112305e)
![Screenshot from 2023-09-14 21-11-41](https://github.com/hamidrezayaghobi/VAE-GAN-DDPM/assets/59170724/44dcb400-cc09-4911-9f2f-b0928e6328d1)
![Screenshot from 2023-09-14 21-11-57](https://github.com/hamidrezayaghobi/VAE-GAN-DDPM/assets/59170724/7143e7ee-06e3-42b7-b437-1064474eaadd)


## Generative Adversarial Network (GAN)
GAN's architecture consists of a Discriminator and a Generator. The Discriminator identifies whether an input image ($x$) is real or generated by the Generator. The Generator tries to create fake images given a random noise ($z$) such that the Discriminator can not distinguish them from the real images. These functionalities give rise to an adversarial situation where these networks can be seen as players. Thus, the standard loss for GAN is the following min-max loss. The Generator tries to minimize it, while the Discriminator tries to maximize it.

$$ \min_{\theta_g} \max_{\theta_d} E_{x \sim p_{data}}[log(D_{\theta_d}(x)] + E_{z \sim p(z)}[log(1 - D_{\theta_d}(G_{\theta_g}(z)))] $$

We try to optimize this loss function by alternating between the following objectives for the Discriminator and the Generator: </br>
Discriminator's objective: </br>

 $$ \max_{\theta_d} E_{x \sim p_{data}}[log(D_{\theta_d}(x)] + E_{z \sim p(z)}[log(1 - D_{\theta_d}(G_{\theta_g}(z)))] $$ 
 </br>
 
Generator's objective: </br>

$$ \min_{\theta_g} E_{z \sim p(z)}[log(1 - D_{\theta_d}(G_{\theta_g}(z)))] $$ 
</br>

Each playerâ€™s cost depends on the parameters of the other player. However, each player can only optimize its own parameters.

### Normal GAN Example
![Screenshot from 2023-09-14 21-17-25](https://github.com/hamidrezayaghobi/VAE-GAN-DDPM/assets/59170724/a19cc84e-e2eb-412c-8442-d66cba7f9112)

### Convolutional GAN Example
![Screenshot from 2023-09-14 21-16-43](https://github.com/hamidrezayaghobi/VAE-GAN-DDPM/assets/59170724/36ccefb4-c644-419c-84a3-8d5393ed7051)


## Denoising diffusion probabilistic models (DDPMs) 
DDPMs learn to generate images by gradual denoising a completely random pattern in a step-by-step process. The model learns a set of diffusion steps that describe how the noise evolves through time and a denoising function that removes the noise at each time step. The denoising function is typically implemented as a neural network that takes in the noisy data and the time step as input and outputs the denoised data. After iterative denoising steps over a time index, an image from the training distribution is generated. The overall procedure of training and sampling from DDPM is represented in the following pseudocodes from the DDPM paper.

### DDPM Examples
![Screenshot from 2023-09-14 21-20-35](https://github.com/hamidrezayaghobi/VAE-GAN-DDPM/assets/59170724/f413bcfa-346b-4b0d-8a59-71cd59421981)
![Screenshot from 2023-09-14 21-20-40](https://github.com/hamidrezayaghobi/VAE-GAN-DDPM/assets/59170724/84114ec6-c8c1-466b-a754-3dc262620dcf)

### Conditional DDPM Examples
![Screenshot from 2023-09-14 21-21-28](https://github.com/hamidrezayaghobi/VAE-GAN-DDPM/assets/59170724/955d1ffa-6457-47a7-a830-2911ec94786b)
![Screenshot from 2023-09-14 21-21-36](https://github.com/hamidrezayaghobi/VAE-GAN-DDPM/assets/59170724/5db79819-db8c-42a9-ad66-b84604603871)


